{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:16:27.831988Z","iopub.status.busy":"2024-09-06T05:16:27.831717Z","iopub.status.idle":"2024-09-06T05:16:29.713141Z","shell.execute_reply":"2024-09-06T05:16:29.712269Z","shell.execute_reply.started":"2024-09-06T05:16:27.831959Z"},"trusted":true},"outputs":[],"source":["## dataset.py\n","\n","import json \n","import pandas as pd\n","\n","class Dataset(object):\n","    def __init__(\n","        self,\n","        dataset_filepath: str,\n","    ):\n","        self.dataset = []\n","        self.dataset = pd.read_csv(dataset_filepath).to_dict('records')\n","        for dp in self.dataset:\n","            if not dp['answer_choices'] or dp['answer_choices'] != dp['answer_choices']:\n","                del dp['answer_choices']\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n","    \n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:16:29.715237Z","iopub.status.busy":"2024-09-06T05:16:29.714908Z","iopub.status.idle":"2024-09-06T05:16:49.648691Z","shell.execute_reply":"2024-09-06T05:16:49.647841Z","shell.execute_reply.started":"2024-09-06T05:16:29.715211Z"},"trusted":true},"outputs":[],"source":["## model/decoder_functions.py\n","\n","import torch\n","import torch.nn.functional as F\n","\n","def decoder_predict_multiple_choice(\n","    transformer, input_tokenizer, target_tokenizer, batch\n","): \n","    tokenized_batch = decoder_tokenize_batch(input_tokenizer, target_tokenizer, batch, transformer.device)\n","    output = transformer(\n","        input_ids=tokenized_batch[\"input_ids\"],\n","        attention_mask=tokenized_batch[\"input_mask\"],\n","        use_cache=True,\n","    )\n","    past_key_values = output.past_key_values\n","\n","    num_answer_choices = (\n","        tokenized_batch[\"answer_choices_ids\"].shape[0]\n","        // tokenized_batch[\"input_mask\"].shape[0]\n","    )\n","\n","    '''\n","    Expand the input_mask and past_key_values since these are the same and can be repeated for the different answer choices within an example \n","    '''\n","\n","    batch_size, max_input_len = tokenized_batch[\"input_mask\"].shape\n","    expanded_input_mask = torch.repeat_interleave(tokenized_batch[\"input_mask\"], num_answer_choices, dim=0)\n","\n","    expanded_past_key_valyes = []\n","    for pastKeyValues_perLayer in past_key_values:\n","        list_broadcast_pastKeyValues_perLayer = []\n","        for key_or_value in pastKeyValues_perLayer:\n","            # This is for keys or values which have dimension [batch_size, max_input_len, num_heads, head_dim]\n","            # This is the standard for Hugging Face.\n","            if len(key_or_value.shape) == 4:\n","                list_broadcast_pastKeyValues_perLayer.append(\n","                    torch.repeat_interleave(key_or_value, num_answer_choices, dim=0)\n","                )\n","            # This is for keys or values which have dimension [batch_size x num_heads, head_dim, max_input_len].\n","            # This is what is used for BLOOM in transformers == 4.22.0\n","            elif len(key_or_value.shape) == 3:\n","                num_heads = key_or_value.shape[0] // batch_size\n","                flatten_keyOrValue = key_or_value.reshape(\n","                    ((batch_size, num_heads) + key_or_value.shape[1:])\n","                )\n","                broadcast_flatten_keyOrValue = torch.repeat_interleave(\n","                    flatten_keyOrValue, num_answer_choices, dim=0\n","                )\n","                list_broadcast_pastKeyValues_perLayer.append(\n","                    broadcast_flatten_keyOrValue.flatten(0, 1)\n","                )\n","            else:\n","                raise ValueError(\n","                    f\"Invalid cached key or value shape: \", key_or_value.shape\n","                )\n","\n","        expanded_past_key_valyes.append(\n","            tuple(list_broadcast_pastKeyValues_perLayer)\n","        )\n","\n","\n","    # Combine the input mask and choice mask so the model knows which cached input representations\n","    # are padded when conditioning on the cached input representations.\n","    # [batch_size x num_choices, max_input_len + max_choice_len]\n","    combined_mask = torch.cat(\n","        [expanded_input_mask, tokenized_batch[\"answer_choices_mask\"]], dim=1\n","    )\n","\n","    # WARNING: The loss at transformer_outputs[0] is not valid, since allChoices_ids uses a\n","    # pad token of 0 and so the loss will not be ignored for the pad tokens\n","    transformer_outputs = transformer(\n","        input_ids=tokenized_batch[\"answer_choices_ids\"],\n","        attention_mask=combined_mask,\n","        past_key_values=expanded_past_key_valyes,\n","        use_cache=True,\n","    )\n","\n","    # We used the logits for all choices to compute the log probs per example since\n","    # the loss returned in transformer_outputs will average the negative log probs across examples\n","    # [batch_size x num_choices, max_choice_len, vocab_size]\n","    answer_choice_ids_logits = transformer_outputs.logits.float()\n","    vocab_size = answer_choice_ids_logits.shape[-1]\n","\n","    # Shift the ids, masks, and logits to handle predicting the next token for the decoder. Note that we need to pass in the input_ids and cannot rely on HuggingFace automatically constructing the ids from the labels, since we need to pass in an attention mask to handle the cached input representations.\n","    shifted_answer_choice_ids_logits = answer_choice_ids_logits[..., :-1, :].contiguous()\n","    shifted_answer_choice_ids = tokenized_batch[\"answer_choices_ids\"][\n","        ..., 1:\n","    ].contiguous()\n","    shifted_answer_choice_masks = tokenized_batch[\"answer_choices_mask\"][\n","        ..., 1:\n","    ].contiguous()\n","\n","    shifted_answer_choices_max_len = shifted_answer_choice_ids_logits.shape[1]\n","    vocab_size = shifted_answer_choice_ids_logits.shape[-1]\n","\n","    # Compute the log probability of the ids for all choices with respect to the logits [batch_size x num_choices x (max_choice_len-1)]\n","    shifted_answer_choice_ids_log_probs = -F.cross_entropy(\n","        shifted_answer_choice_ids_logits.view(-1, vocab_size),\n","        shifted_answer_choice_ids.view(-1),\n","        reduction=\"none\",\n","    )\n","\n","\n","    # [batch_size, num_answer_choices, answer_choices_max_len]\n","    shifted_answer_choice_ids_log_probs = shifted_answer_choice_ids_log_probs.reshape(\n","        -1, num_answer_choices, shifted_answer_choices_max_len\n","    )\n","\n","    shifted_answer_choices_mask = shifted_answer_choice_masks.reshape(\n","        -1, num_answer_choices, shifted_answer_choices_max_len\n","    )\n","\n","    answer_choice_log_probs = torch.sum(shifted_answer_choice_ids_log_probs * shifted_answer_choices_mask, dim=2)\n","\n","    _, predicted_choice = torch.max(answer_choice_log_probs, dim=1)\n","\n","    return predicted_choice, answer_choice_log_probs\n","\n","\n","\n","def decoder_generate(\n","    transformer, \n","    input_tokenizer,\n","    target_tokenizer,\n","    batch,\n","    max_gen_len\n","):\n","    tokenized_batch = decoder_tokenize_batch(input_tokenizer, target_tokenizer, batch, transformer.device)\n","\n","    generation_output = transformer.generate(\n","        input_ids=tokenized_batch[\"input_ids\"],\n","        attention_mask=tokenized_batch[\"input_mask\"],\n","        max_new_tokens=max_gen_len,\n","        eos_token_id=input_tokenizer.eos_token_id,\n","        pad_token_id=input_tokenizer.pad_token_id,\n","        bos_token_id=input_tokenizer.bos_token_id,\n","        do_sample=False,\n","        return_dict_in_generate=True,\n","    )\n","\n","    # Remove the original input ids from the generated ids to get just the generated ids \n","    input_len = tokenized_batch[f\"input_ids\"].shape[-1]\n","\n","    generated_ids = generation_output[\"sequences\"][:, input_len:]\n","\n","    generated_txt = input_tokenizer.batch_decode(\n","        generated_ids, skip_special_tokens=True\n","    )\n","\n","    return generation_output[\"sequences\"].cpu().numpy().tolist(), generated_txt\n","\n","def decoder_tokenize_batch(input_tokenizer, target_tokenizer, batch, device):\n","\n","    tokenized_batch = {}\n","\n","    keys_to_tokenize_with_tokenizer = [(\"input\", input_tokenizer), (\"answer_choices\", target_tokenizer), (\"target\", target_tokenizer)]\n","\n","\n","    # Tokenize keys which should be tokenized\n","    for key, tokenizer in keys_to_tokenize_with_tokenizer:\n","        if key in batch:\n","            # The values of the batch are normally a list of text.The exception is that for answer_choices, the values  is a list of list. We flatten this to a single list to pass is into the tokenizer \n","            if key == \"answer_choices\":\n","#                 print(batch[key])\n","                text = [item for list in batch[key] for item in list]\n","            else:\n","                text = batch[key]\n","\n","        tokenized_dict = tokenizer(\n","            text,\n","            return_tensors=\"pt\",\n","            padding=\"longest\",\n","            truncation=\"longest_first\",\n","        )\n","\n","        input_ids = tokenized_dict[\"input_ids\"]\n","        attention_mask = tokenized_dict[\"attention_mask\"]\n","\n","        if device is not None:\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","\n","        tokenized_batch[f\"{key}_ids\"] = input_ids\n","        tokenized_batch[f\"{key}_mask\"] = attention_mask\n","    return tokenized_batch"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:16:49.650057Z","iopub.status.busy":"2024-09-06T05:16:49.649735Z","iopub.status.idle":"2024-09-06T05:16:49.665882Z","shell.execute_reply":"2024-09-06T05:16:49.665154Z","shell.execute_reply.started":"2024-09-06T05:16:49.650032Z"},"trusted":true},"outputs":[],"source":["## model/encoder_decoder_functions.py\n","\n","import torch\n","import torch.nn.functional as F\n","\n","\n","def compute_loss(transformer, tokenizer, batch):\n","\n","    tokenized_batch = encoder_decoder_tokenize_batch(tokenizer, batch, transformer.device)\n","\n","    transformer_outputs = transformer(\n","        input_ids=tokenized_batch[\"input_ids\"],\n","        attention_mask=tokenized_batch[\"input_mask\"],\n","        labels=tokenized_batch[\"target_ids\"],\n","    )\n","\n","    # [batch_size, max_target_len, vocab_size]\n","    target_logits = transformer_outputs[1].float()\n","    vocab_size = target_logits.shape[-1]\n","\n","    # Compute the log probability of the ids for all choices with respect to the logits\n","    # [batch_size x max_target_len]\n","    negative_log_probs = F.cross_entropy(\n","        target_logits.reshape(-1, vocab_size),\n","        tokenized_batch[\"target_ids\"].reshape(-1),\n","        reduction=\"none\",\n","    )\n","\n","    # Zero out log_probs for target_ids with no loss\n","    target_mask = tokenized_batch[\"target_mask\"].reshape(-1)\n","    \n","    \n","    sum_negative_log_prob = torch.sum(\n","        negative_log_probs * target_mask\n","    )\n","\n","    loss = sum_negative_log_prob / torch.sum(\n","            target_mask\n","        )\n","\n","    return loss\n","\n","def encoder_decoder_predict_multiple_choice(\n","    transformer, tokenizer, batch\n","):\n","    tokenized_batch = encoder_decoder_tokenize_batch(tokenizer, batch, transformer.device)\n","#     print(tokenized_batch)\n","\n","    encoder_outputs = transformer.get_encoder()(\n","        tokenized_batch[\"input_ids\"],\n","        tokenized_batch[\"input_mask\"],\n","    )\n","\n","    # The answer_choices is the flattened batch of answer choices. To get the number of answer choices per example, we divide the total number of answer choices in a batch by the batch size. \n","    num_answer_choices = (\n","        tokenized_batch[\"answer_choices_ids\"].shape[0] // tokenized_batch[\"input_mask\"].shape[0]\n","    )\n","\n","    '''Expand the input_mask and encoder_outputs since these are the same and can be repeated for the different answer choices within an example \n","    '''\n","    # [batch_size x num_choices, max_input_len]\n","    expanded_input_mask = torch.repeat_interleave(tokenized_batch[\"input_mask\"], num_answer_choices, dim=0)\n","    # BaseModelOutput object from HuggingFace where the first element is the hidden states of the encoder at the last layer \n","    # [batch_size x num_choices, max_input_len, ff_dim]\n","    expanded_encoder_outputs = (\n","        torch.repeat_interleave(encoder_outputs[0], num_answer_choices, dim=0),\n","    )\n","\n","\n","    # WARNING: The loss at transformer_outputs[0] is not valid, since answer_choices_ids uses a pad token of 0 (while loss expects a pad token of -100) so the loss will not be ignored for the pad tokens. \n","    # The input mask is passed in for the cross encoder-decoder attention.\n","    transformer_outputs = transformer(\n","        attention_mask=expanded_input_mask,\n","        encoder_outputs=expanded_encoder_outputs,\n","        labels=tokenized_batch[\"answer_choices_ids\"],\n","    )\n","\n","    # We used the logits for all choices to compute the log probs per example since the loss returned in transformer_outputs will average the negative log probs across examples\n","    # [batch_size x num_choices, max_choice_len, vocab_size]\n","    answer_choice_ids_logits = transformer_outputs[1].float()\n","    answer_choices_max_len = answer_choice_ids_logits.shape[1]\n","    vocab_size = answer_choice_ids_logits.shape[-1]\n","\n","    # Compute the log probability of the ids for all choices with respect to the logits\n","    # [batch_size x num_choices x max_choice_len]\n","    answer_choices_ids_log_probs = -F.cross_entropy(\n","        answer_choice_ids_logits.view(-1, vocab_size),\n","        tokenized_batch[\"answer_choices_ids\"].view(-1),\n","        reduction=\"none\",\n","    )\n","\n","    # [batch_size, num_answer_choices, answer_choices_max_len]\n","    answer_choices_ids_log_probs = answer_choices_ids_log_probs.reshape(\n","        -1, num_answer_choices, answer_choices_max_len\n","    )\n","\n","    answer_choices_mask = tokenized_batch[\"answer_choices_mask\"].reshape(\n","        -1, num_answer_choices, answer_choices_max_len\n","    )\n","\n","    answer_choice_log_probs = torch.sum(answer_choices_ids_log_probs * answer_choices_mask, dim=2)\n","\n","    _, predicted_choice = torch.max(answer_choice_log_probs, dim=1)\n","\n","    return predicted_choice, answer_choice_log_probs\n","\n","\n","def encoder_decoder_generate(\n","    transformer, \n","    tokenizer,\n","    batch,\n","    max_gen_len\n","):\n","    tokenized_batch = encoder_decoder_tokenize_batch(tokenizer, batch, transformer.device)\n","\n","    generation_output = transformer.generate(\n","        input_ids=tokenized_batch[\"input_ids\"],\n","        attention_mask=tokenized_batch[\"input_mask\"],\n","        max_new_tokens=max_gen_len,\n","        eos_token_id=tokenizer.eos_token_id,\n","        pad_token_id=tokenizer.pad_token_id,\n","        bos_token_id=tokenizer.bos_token_id,\n","        do_sample=False,\n","        return_dict_in_generate=True,\n","    )\n","    generated_txt = tokenizer.batch_decode(\n","        generation_output[\"sequences\"], skip_special_tokens=True\n","    )\n","\n","    return generation_output[\"sequences\"].cpu().numpy().tolist(), generated_txt\n","\n","def encoder_decoder_tokenize_batch(tokenizer, batch, device):        \n","\n","    tokenized_batch = {}\n","\n","    # encoder decoder models pad to the right \n","    tokenizer.padding_side = \"right\"\n","\n","    keys_to_tokenize = [\"input\", \"answer_choices\", \"target\"]\n","\n","    for key in keys_to_tokenize:\n","        if key in batch:\n","            # The values of the batch are normally a list of text.The exception is that for answer_choices, the values  is a list of list. We flatten this to a single list to pass is into the tokenizer \n","            if key == \"answer_choices\":\n","#                 print(batch[key])\n","                text = [item for list in batch[key] for item in list]\n","            else:\n","                text = batch[key]\n","\n","            tokenized_dict = tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                padding=\"longest\",\n","                truncation=\"longest_first\",\n","            )\n","\n","            input_ids = tokenized_dict[\"input_ids\"]\n","            attention_mask = tokenized_dict[\"attention_mask\"]\n","\n","            if device is not None:\n","                input_ids = input_ids.to(device)\n","                attention_mask = attention_mask.to(device)\n","\n","            tokenized_batch[f\"{key}_ids\"] = input_ids\n","            tokenized_batch[f\"{key}_mask\"] = attention_mask\n","\n","    return tokenized_batch\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:16:49.667324Z","iopub.status.busy":"2024-09-06T05:16:49.667087Z","iopub.status.idle":"2024-09-06T05:17:02.840873Z","shell.execute_reply":"2024-09-06T05:17:02.839555Z","shell.execute_reply.started":"2024-09-06T05:16:49.667301Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: peft in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (24.1)\n","Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (6.0.0)\n","Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (2.2.1+cu121)\n","Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (4.45.2)\n","Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (4.66.5)\n","Requirement already satisfied: accelerate>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (1.0.1)\n","Requirement already satisfied: safetensors in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (0.4.5)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from peft) (0.26.1)\n","Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n","Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\n","Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.6.68)\n","Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers->peft) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers->peft) (0.20.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"]}],"source":["## merges/Merges.py\n","! pip install peft\n","import copy\n","import os\n","\n","from peft import load_peft_weights, PeftConfig\n","from safetensors.torch import save_file\n","\n","\n","from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoModelForCausalLM,\n","    AutoTokenizer\n",")\n","\n","class Merges(object):\n","\n","    def __init__(self, name):\n","        self.name = name\n","\n","        self.list_models = None\n","         \n","        self.loaded_models = None\n","        self.loaded_configs = None\n","        self.base_model = None\n","        self.tokenizer = None\n","        self.input_tokenizer = None\n","        self.target_tokenizer = None\n","\n","        self.base_model_name = None\n","        self.base_model_revision_id = None\n","\n","        self.max_seq_len = None\n","        self.max_gen_len = None\n","\n","        self.device = None\n","        self.architecture = None\n","\n","        self.merged_model = None\n","\n","    def get_name(self):\n","        return self.name\n","\n","    def get_model_config(self):\n","        raise NotImplementedError\n","\n","    def _load_base_model(self):\n","        if self.architecture == \"encoder_decoder\":\n","            self.base_model =  AutoModelForSeq2SeqLM.from_pretrained(self.base_model_name, revision=self.base_model_revision_id, token=os.environ[\"HF_AUTH_TOKEN\"]).to(self.device)\n","        elif self.architecture == \"decoder\":\n","            self.base_model =  AutoModelForCausalLM.from_pretrained(self.base_model_name, revision=self.base_model_revision_id, token=os.environ[\"HF_AUTH_TOKEN\"]).to(self.device)\n","        else:\n","            raise NotImplementedError(f\"Architecture not implemented {self.architecture}\")\n","        \n","\n","    def _load_tokenizer(self):\n","\n","        if self.architecture == \"encoder_decoder\":\n","            if self.tokenizer is None:\n","\n","                self.tokenizer = AutoTokenizer.from_pretrained(\n","                    self.base_model_name,\n","                    revision=self.base_model_revision_id,\n","                    model_max_length=self.max_seq_len,\n","                    legacy=False,\n","                    token=os.environ[\"HF_AUTH_TOKEN\"]\n","                )\n","\n","        elif self.architecture == \"decoder\":\n","            if self.input_tokenizer is None or self.target_tokenizer is None:\n","                    \n","                self.input_tokenizer = AutoTokenizer.from_pretrained(\n","                    self.base_model_name,\n","                    revision=self.base_model_revision_id,\n","                    model_max_length=self.max_seq_len,\n","                    legacy=False,\n","                    token=os.environ[\"HF_AUTH_TOKEN\"]\n","                )\n","                self.target_tokenizer = copy.deepcopy(self.input_tokenizer)\n","\n","                # Use eos_token for pad_token if it doesn't exist. This is ok since the\n","                # pad tokens will be ignored through the mask\n","                if self.input_tokenizer.pad_token_id is None:\n","                    self.input_tokenizer.pad_token_id = self.input_tokenizer.eos_token_id\n","                if self.target_tokenizer.pad_token_id is None:\n","                    self.target_tokenizer.pad_token_id = self.target_tokenizer.eos_token_id\n","\n","                # Add BOS and not EOS token \n","                self.input_tokenizer.padding_side = \"left\"\n","\n","                # Add EOS and not BOS token \n","                self.target_tokenizer.padding_side = \"right\"\n","                self.target_tokenizer.add_bos_token = False\n","                self.target_tokenizer.add_eos_token = True\n","        else:\n","            raise NotImplementedError(f\"Architecture not implemented {self.architecture}\")\n","\n","    def predict_multiple_choice(self, batch):\n","        assert self.base_model is not None\n","        if self.architecture == \"encoder_decoder\":\n","            assert self.tokenizer is not None\n","            return encoder_decoder_predict_multiple_choice(self.base_model, self.tokenizer, batch)\n","        elif self.architecture == \"decoder\":\n","            return decoder_predict_multiple_choice(self.base_model, self.input_tokenizer, self.target_tokenizer, batch)\n","        else:\n","            raise NotImplementedError(f\"Architecture not implemented {self.architecture}\")\n","    \n","    def generate(self, batch):\n","        assert self.base_model is not None\n","        if self.architecture == \"encoder_decoder\":\n","            assert self.tokenizer is not None\n","            return encoder_decoder_generate(self.base_model, self.tokenizer, batch, self.max_gen_len)\n","        elif self.architecture == \"decoder\":\n","            return decoder_generate(self.base_model, self.input_tokenizer, self.target_tokenizer, batch, self.max_gen_len)\n","        else:\n","            raise NotImplementedError(f\"Architecture not implemented {self.architecture}\")\n","\n","    def _load_huggingface_models_and_configs(self):\n","        assert len(self.list_models) > 0, f\"List of models must include at leat 1 model\"\n","\n","        parameter_names = None\n","        for model_name, revision_id in self.list_models:\n","\n","            peft_model_parameters = load_peft_weights(model_name, revision=revision_id, token=os.environ[\"HF_AUTH_TOKEN\"])\n","            peft_config = PeftConfig.from_pretrained(model_name)\n","\n","            if parameter_names is None:\n","                parameter_names = set(peft_model_parameters.keys())\n","\n","            if parameter_names != set(peft_model_parameters.keys()):\n","                print(f\"WARNING: parameters in {model_name} do not match {self.list_models[0]}\")\n","\n","            self.loaded_models[model_name] = peft_model_parameters \n","            self.loaded_configs[model_name] = peft_config\n","\n","    def merge(\n","        self,\n","    ):\n","        raise NotImplementedError\n","    \n","    def save_model(self, output_dir):\n","        assert self.merged_model is not None, \"Merged model is empty\"\n","        assert len(self.merged_model) > 0, \"Merged model is empty\"\n","        # Save merged model as safetensor \n","        torch.save(self.merged_model, os.path.join(output_dir, \"safetensor.pt\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:17:02.845018Z","iopub.status.busy":"2024-09-06T05:17:02.844237Z","iopub.status.idle":"2024-09-06T05:17:02.855950Z","shell.execute_reply":"2024-09-06T05:17:02.855336Z","shell.execute_reply.started":"2024-09-06T05:17:02.844976Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:17:02.857031Z","iopub.status.busy":"2024-09-06T05:17:02.856812Z","iopub.status.idle":"2024-09-06T05:17:02.873815Z","shell.execute_reply":"2024-09-06T05:17:02.873122Z","shell.execute_reply.started":"2024-09-06T05:17:02.857009Z"},"trusted":true},"outputs":[],"source":["import torch\n","from typing import List, Tuple, Dict, Any\n","from peft import get_peft_model, set_peft_model_state_dict\n","\n","class FlanT5GeoMed(Merges):\n","    def __init__(self, name: str):  \n","        super().__init__(name)\n","\n","        # List of models to load for the merge\n","        self.list_models: List[Tuple[str, str]] = [\n","            # Text Classification\n","            (\"lorahub/flan_t5_xl-dbpedia_14_given_list_what_category_does_the_paragraph_belong_to\", \"883db61b41a3a9e8716f5391d782f653fd9d693b\"),\n","            (\"lorahub/flan_t5_xl-wiki_qa_Topic_Prediction_Question_Only\", \"cc024699f37aee24e72cd28a596dbf3451a93484\"),\n","\n","            # Question Answering\n","            (\"lorahub/flan_t5_xl-anli_r2\", \"ea7872e79fddc6e9df57b88c429bdb283b414bea\"),\n","            (\"lorahub/flan_t5_xl-web_questions_question_answer\", \"37701f6f673974308517151387182f42271a2eab\"),\n","            (\"lorahub/flan_t5_xl-duorc_SelfRC_question_answering\", \"b56b5b0b72a0a4b90b120833ff466aa7ef85dd84\"),\n","            (\"lorahub/flan_t5_xl-adversarial_qa_dbert_question_context_answer\", \"a935c63c0c7deaca77f437efd3425192a88dd90e\"),\n","\n","            # Text Generation\n","            (\"lorahub/flan_t5_xl-gem_e2e_nlg\", \"04e25c5739d151e42916b262cb0ee900aa854816\"),\n","\n","            # Text2Text Generation\n","            (\"lorahub/flan_t5_xl-wiki_hop_original_explain_relation\", \"d6bdec80c60d55db0b7125f8ca0d02871ab3ab34\"),\n","            (\"lorahub/flan_t5_xl-duorc_SelfRC_title_generation\", \"17653e0c744bb1453f93b816d1eb140d991be6a4\"),\n","\n","            # Sentence Similarity\n","            (\"lorahub/flan_t5_xl-glue_mrpc\", \"292a6f0c2dec34a9faa143b37dc734eee14c860a\"),\n","            (\"lorahub/flan_t5_xl-glue_cola\", \"7fef5d273d145e26b07762b43abcbaa83874dc23\"),\n","\n","            # Comprehension and Understanding Tasks\n","            (\"lorahub/flan_t5_xl-wiki_bio_comprehension\", \"9d06f885dbbbe69327203b299193873ea281522c\"),\n","            (\"lorahub/flan_t5_xl-wiki_bio_key_content\", \"f98ee1718a9ce23446671023a60fb05a57f5e9d3\"),\n","            (\"lorahub/flan_t5_xl-wiki_bio_guess_person\", \"e8998f9f0fad7aef94408c4741e7fbe2ff11f79d\"),\n","            (\"lorahub/flan_t5_xl-wiki_bio_who\", \"c081565f0d3e3aa251fa9d44fc6678d70cc9e20f\"),\n","\n","            # Search and Retrieval\n","            (\"lorahub/flan_t5_xl-wiki_qa_found_on_google\", \"cb5c59ee688f22e0314968e2a0c1bee692e66c27\"),\n","\n","            # Natural Language Generation\n","            (\"lorahub/flan_t5_xl-gem_web_nlg_en\", \"8043f44956456dffb6cc5e07bc59bffdf618ac97\"),\n","\n","            # Paraphrasing and Extraction\n","            (\"lorahub/flan_t5_xl-duorc_ParaphraseRC_extract_answer\", \"c008dacf47c7836a0bcd2d4c47cd27923d2cda1e\"),\n","            (\"lorahub/flan_t5_xl-duorc_SelfRC_extract_answer\", \"377a71b7c71099688c836d7417eb9cfc0c33f6b5\"),\n","\n","            # Process Understanding\n","            (\"lorahub/flan_t5_xl-wiqa_what_might_be_the_last_step_of_the_process\", \"fea37d25cf4eb8d81a85fc3296e7781fc8ea10db\"),\n","        ]\n","        \n","        # Hyperparameters\n","        self.base_model_name: str = \"google/flan-t5-xl\" #base model\n","        self.base_model_revision_id: str = \"7d6315df2c2fb742f0f5b556879d730926ca9001\"\n","        self.is_peft: bool = True\n","        self.max_seq_len: int = 512\n","        self.device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","        self.architecture: str = \"encoder_decoder\"\n","\n","        self.loaded_models: Dict[str, Dict[str, torch.Tensor]] = {}\n","        self.loaded_configs: Dict[str, Any] = {}\n","        self.merged_model: Dict[str, torch.Tensor] = {}\n","\n","    def merge(self):\n","        ''' Load HuggingFace checkpoints and configs '''\n","        super()._load_huggingface_models_and_configs()\n","\n","        ''' Load base model and tokenizer '''\n","        self._load_base_model()\n","        self._load_tokenizer()\n","\n","        if not self.loaded_models:\n","            raise ValueError(\"No models loaded for merging.\")\n","\n","        base_model_params = self.base_model.state_dict()\n","        \n","        base_flattened_vector = torch.cat([param.to(self.device).view(-1) for param in base_model_params.values()])\n","\n","        all_models = list(self.loaded_models.values())\n","        merged_model_dict = self.process_models(base_model_params, all_models)\n","\n","        self.merged_model = merged_model_dict\n","            \n","        assert len(self.merged_model) > 0, \"Merged model is empty\"\n","\n","        self.base_model.load_state_dict(self.merged_model, strict=False)  # Load parameters from the merged model dict\n","        self.base_model.eval()  # Set to evaluation mode\n","        return self.base_model\n","\n","    def process_models(self, base_model_params: Dict[str, torch.Tensor], all_models: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n","        for param_name, param in base_model_params.items():  \n","            # Skip \".k.\" and \".o.\" parameters\n","            if any(substring in param_name for substring in [\".q.\", \".v.\"]):\n","                task_vector, original_shape = self.process_task_vectors(param_name, param, all_models)\n","                if task_vector:\n","                    result_for_block = self.compute_geometric_median(task_vector)\n","                    result_for_block_rescaled = result_for_block.view(original_shape)\n","                    base_model_params[param_name] += result_for_block_rescaled\n","            else:\n","                continue\n","\n","        return base_model_params\n","\n","    def process_task_vectors(self, param_name: str, param: torch.Tensor, all_models: List[Dict[str, torch.Tensor]]) -> Tuple[List[torch.Tensor], torch.Size]:\n","        task_vector = []\n","        original_shape = None\n","\n","        for model in all_models:\n","            vector_A, vector_B = None, None\n","            stir_q, stir_v = \"\", \"\"\n","            \n","            for fine_tuned_param_name, fine_tuned_param in model.items():\n","                index_q = param_name.find(\".q.\")\n","                index_v = param_name.find(\".v.\")\n","\n","                if index_q != -1:\n","                    stir_q = param_name[:index_q+3]\n","                    stir_v = \"\"\n","                if index_v != -1:\n","                    stir_v = param_name[:index_v+3]\n","                    stir_q = \"\"\n","\n","                if \".q.\" in fine_tuned_param_name and stir_q in fine_tuned_param_name:\n","                    if \"lora_A.\" in fine_tuned_param_name:\n","                        vector_A = fine_tuned_param\n","                    elif \"lora_B.\" in fine_tuned_param_name:\n","                        vector_B = fine_tuned_param\n","                elif \".v.\" in fine_tuned_param_name and stir_v in fine_tuned_param_name:\n","                    if \"lora_A.\" in fine_tuned_param_name:\n","                        vector_A = fine_tuned_param\n","                    elif \"lora_B.\" in fine_tuned_param_name:\n","                        vector_B = fine_tuned_param\n","\n","            if vector_A is not None and vector_B is not None:\n","                result = torch.matmul(vector_B, vector_A)\n","                original_shape = result.shape\n","                flattened_model_vector = result.view(-1)\n","                task_vector.append(flattened_model_vector)\n","\n","        return task_vector, original_shape\n","\n","    def compute_geometric_median(self, task_vectors: List[torch.Tensor], eps: float = 1e-8, max_iter: int = 300) -> torch.Tensor:\n","        if not task_vectors:\n","            raise ValueError(\"No task vectors provided for geometric median computation.\")\n","\n","        median = torch.mean(torch.stack(task_vectors), dim=0).to(self.device)\n","\n","        for iteration in range(max_iter):\n","            distances = torch.stack([torch.norm(tv - median) for tv in task_vectors])\n","            distances[distances < 1e-10] = 1e-10  # Avoid division by zero\n","            weights = 1.0 / distances\n","            weights_sum = torch.sum(weights)\n","\n","            if weights_sum == 0:\n","                break\n","\n","            weighted_sum = torch.stack([w * tv for w, tv in zip(weights, task_vectors)]).sum(dim=0)\n","            new_median = weighted_sum / weights_sum\n","            shift = torch.norm(new_median - median)\n","\n","            if shift < eps:\n","                return new_median\n","\n","            median = new_median\n","\n","        return median\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:17:22.268205Z","iopub.status.busy":"2024-09-06T05:17:22.267814Z","iopub.status.idle":"2024-09-06T05:17:42.803668Z","shell.execute_reply":"2024-09-06T05:17:42.802847Z","shell.execute_reply.started":"2024-09-06T05:17:22.268177Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: evaluate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.4.3)\n","Requirement already satisfied: datasets>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (3.0.1)\n","Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (4.66.5)\n","Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.26.1)\n","Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (24.1)\n","Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n","Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n","Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.11)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"]}],"source":["## evaluate.py\n","! pip install evaluate\n","\n","import evaluate \n","import json \n","import os \n","import pandas as pd\n","\n","from typing import List, Dict, Any\n","\n","import torch\n","from tqdm import tqdm\n","from torch.utils import data\n","\n","\n","def convert_dict_of_lists_to_list_of_dicts(dict_of_lists: Dict[Any, List]) -> List[Dict]:\n","    \"\"\"\n","    Args:\n","        dict_of_lists:\n","\n","    Returns:\n","        list_ofDict\n","    \"\"\"\n","    list_of_dicts = []\n","    for datapoint_values in zip(*dict_of_lists.values()):\n","        list_of_dicts.append(dict(zip(dict_of_lists, datapoint_values)))\n","    return list_of_dicts\n","\n","def collate_fn(batch_of_datapoints: List[Dict]) -> Dict[Any, List]:\n","    \"\"\"\n","    Convert a batch of datapoints into a datapoint that is batched. This is meant to override the default collate function in pytorch and specifically can handle when the value is a list \n","\n","    Args:\n","        batch_ofDatapoints:\n","\n","    Returns:\n","\n","    \"\"\"\n","    datapoint_batched = {}\n","    for datapoint in batch_of_datapoints:\n","        # Gather together all the values per key\n","        for key, value in datapoint.items():\n","            if key in datapoint_batched:\n","                datapoint_batched[key].append(value)\n","            else:\n","                datapoint_batched[key] = [value]\n","    return datapoint_batched\n","\n","\n","def evaluate_dataset(\n","    merge_method,\n","    dataset_filepath: str,\n",") -> (Dict, List):\n","\n","    data_loader = data.DataLoader(\n","        Dataset(dataset_filepath),\n","        batch_size=1,\n","        num_workers=0,\n","        shuffle=False,\n","        collate_fn=collate_fn\n","    )\n","\n","    all_batches = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader):\n","            # There are two types of evaluation models:\n","            # 1) multiple choice where the model scores each choice and predicts the choice with the highest score \n","            # 2) generation where the model generate some output give some input \n","            eval_type = batch[\"eval_type\"][0]\n","            \n","            if eval_type == \"multiple_choice\":\n","                (\n","                    predicted_choice,\n","                    answer_choice_scores,\n","                ) = merge_method.predict_multiple_choice(batch)\n","\n","                batch[\"prediction\"] = str(predicted_choice.cpu().numpy().tolist()[0])\n","                all_batches.extend(convert_dict_of_lists_to_list_of_dicts(batch))\n","            \n","            else:\n","                assert eval_type == \"generation\"\n","                (\n","                    generated_ids, generated_txt\n","                ) = merge_method.generate(batch\n","                )\n","                batch[\"prediction\"] = generated_txt \n","                all_batches.extend(convert_dict_of_lists_to_list_of_dicts(batch))\n","\n","    return all_batches\n","\n","def evaluate_model(\n","    merge_method,\n","    all_dataset_filepaths: List[str],\n",") -> Dict:   \n","    output_dir = os.path.join(\"output\", merge_method.get_name())\n","    prediction_dir = os.path.join(output_dir, \"predictions\")\n","    os.makedirs(prediction_dir, exist_ok=True)\n","    # Save merged model \n","    merge_method.save_model(output_dir)\n","\n","    all_scores = {}\n","\n","    for dataset_filepath in all_dataset_filepaths:\n","        dataset_predictions = evaluate_dataset(merge_method, dataset_filepath)\n","        dp_df = pd.DataFrame(dataset_predictions)\n","        dp_df[\"dummy_field\"] = 0\n","        dp_df.to_csv(\"../submission.csv\", columns=[\"id\", \"prediction\", \"dummy_field\"], index=False)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:17:42.805213Z","iopub.status.busy":"2024-09-06T05:17:42.804690Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4217e910ad5541b59f1c4459763afebf","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 2048)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 2048)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=2048, out_features=2048, bias=False)\n","              (k): Linear(in_features=2048, out_features=2048, bias=False)\n","              (v): Linear(in_features=2048, out_features=2048, bias=False)\n","              (o): Linear(in_features=2048, out_features=2048, bias=False)\n","              (relative_attention_bias): Embedding(32, 32)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n","              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n","              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-23): 23 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=2048, out_features=2048, bias=False)\n","              (k): Linear(in_features=2048, out_features=2048, bias=False)\n","              (v): Linear(in_features=2048, out_features=2048, bias=False)\n","              (o): Linear(in_features=2048, out_features=2048, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n","              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n","              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 2048)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=2048, out_features=2048, bias=False)\n","              (k): Linear(in_features=2048, out_features=2048, bias=False)\n","              (v): Linear(in_features=2048, out_features=2048, bias=False)\n","              (o): Linear(in_features=2048, out_features=2048, bias=False)\n","              (relative_attention_bias): Embedding(32, 32)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=2048, out_features=2048, bias=False)\n","              (k): Linear(in_features=2048, out_features=2048, bias=False)\n","              (v): Linear(in_features=2048, out_features=2048, bias=False)\n","              (o): Linear(in_features=2048, out_features=2048, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n","              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n","              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-23): 23 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=2048, out_features=2048, bias=False)\n","              (k): Linear(in_features=2048, out_features=2048, bias=False)\n","              (v): Linear(in_features=2048, out_features=2048, bias=False)\n","              (o): Linear(in_features=2048, out_features=2048, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=2048, out_features=2048, bias=False)\n","              (k): Linear(in_features=2048, out_features=2048, bias=False)\n","              (v): Linear(in_features=2048, out_features=2048, bias=False)\n","              (o): Linear(in_features=2048, out_features=2048, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n","              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n","              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["## main.py\n","\n","def all_merge_handlers():\n","    \"\"\"Enumerate and Load (import) all merge methods.\"\"\"\n","    loaded_merges = {\n","        \"flant5_geomed\": FlanT5GeoMed,\n","    }\n","    \n","    \n","    return loaded_merges\n","\n","# Load correct merging method \n","merging_method = \"flant5_geomed\"\n","os.environ[\"HF_AUTH_TOKEN\"] = \"\"#TO DO - Enter your HF auth token\n","loaded_merges = all_merge_handlers()\n","merge_method = loaded_merges[merging_method](merging_method)\n","\n","# Call the merge function. The merged model is stored under merging_method object \n","merge_method.merge()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/200 [00:00<?, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","100%|| 200/200 [01:31<00:00,  2.18it/s]\n"]}],"source":["\n","dataset_filepaths = [\"..data/validation.csv\"] #To Enter Path Of Validation Dataset\n","# Evaluate method on datsets passed in (used for testing)\n","evaluate_model(\n","    merge_method,\n","    dataset_filepaths,\n",")\n"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"databundleVersionId":9317659,"sourceId":82622,"sourceType":"competition"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
